TO DO
-----
Fix this when using TF2 on AWS EC2 instance:
    2020-06-12 03:17:21.969689: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
    Traceback (most recent call last):
      File "example.py", line 373, in <module>
        losses, accuracies = evaluate(args.which_set, args.prune_strategy, args.dwr, args.epochs, output_dir)
      File "example.py", line 269, in evaluate
        pruner = lottery_ticket_pruner.LotteryTicketPruner(model)
      File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/lottery_ticket_pruner/lottery_ticket_pruner.py", line 249, in __init__
        if self._prunable(layer, weights):
      File "/home/ubuntu/anaconda3/lib/python3.7/site-packages/lottery_ticket_pruner/lottery_ticket_pruner.py", line 285, in _prunable
        keras.layers.Convolution2DTranspose,
    AttributeError: module 'keras.layers' has no attribute 'Convolution2DTranspose'

Create shell scripts to run complete matrix of example.py
Finish README
Delete TODO.txt
Bump version to legit first version
Change API token to one scoped to only lottery_ticket_pruner repo
Implement same-sign pruning


DONE
----
Get "pip install" working
    - Basic install
    - Create wheel
    - Add testing support
Enable code coverage report generation
Find better solution than passing around 2 different kinds of prune functions (local and global)
Inspect code coverage misses
Make sure unit tests pass reliably even after multiple invocations
Make sure code can be imported intelligently. "import lottery_ticker_pruner; ltp = LotteryTicketPruner(model)"
Make sure we're doing logging in interesting places
Document the code thoroughly
Figure out why tox isn't showing proper code coverage numbers but `pytest --cov=lottery_ticket_pruner --cov-branch --cov-append --cov-report=term --pyargs lottery_ticket_pruner tests` is
Add copyrights to code
Create example (using MNIST?)
Removed restore_initial_weights() in favor of caller doing model.get_weights(), model.set_weights()
Include original weights in results returned by iterate_prunables()
Consider changing prune_weights() to calc_prune_mask() and apply_pruning() to apply_pruning(model)
Implement 'large_final' pruning strategy
Implement Dynamic Weight Rescaling
Document the example better
Review what all I should leave copyright in and what I shouldn't.
Also check to see if including MIT license in the files is a good idea.
    ANSWER: A single LICENSE file at root is sufficient.
Train using patience setting for paper? Or arbitrary # of epochs?
    ANSWER: Added support for patience callback. Using patience=3 in examples.
See if pruning from transfer learning starting points instead of random weights has benefits
    ANSWER: Yes, it definitely applies to transfer learning too.
Set flake8 max line length to 120 and have it enforce. Reformat code accordingly as needed.
Update example.py from transfer_learning_example.py
Address all outstanding TODOs
Implement inside my training pipeline to see if pruning results for DNNs show promise
Use github to build upon pushes?
Post package to pypi
    Keyword: Keras, pruned, CNN pruning, DNN pruning, lottery ticket pruning
    https://packaging.python.org/guides/using-testpypi/
Make sure package works with python 3.6 thru 3.8
Make sure things work with TF2
Hook up posting to pypi.org
