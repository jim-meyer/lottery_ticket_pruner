TO DO
-----
Finish README
    ec2_instance_ip=
    scp -i ~/.ssh/MLProjectsKeyPair.pem /cygdrive/c/git/lottery_ticket_pruner/dist/lottery_ticket_pruner-0.0.1a13-py2.py3-none-any.whl ubuntu@${ec2_instance_ip}:/home/ubuntu
    ssh -i ~/.ssh/MLProjectsKeyPair.pem ubuntu@${ec2_instance_ip} mkdir ltp
    scp -r -i ~/.ssh/MLProjectsKeyPair.pem /cygdrive/c/git/lottery_ticket_pruner/examples/ ubuntu@${ec2_instance_ip}:/home/ubuntu/ltp

    ssh -i ~/.ssh/MLProjectsKeyPair.pem ubuntu@${ec2_instance_ip}
    source /home/ubuntu/anaconda3/bin/activate tensorflow_p36
    pip install --no-deps lottery_ticket_pruner-0.0.1a13-py2.py3-none-any.whl
    cd ltp/examples
    chmod +x example.sh
    timestamp=$(date +%d-%m-%Y-%H-%M-%S)
    aws s3api put-object --bucket lottery-ticket-pruner --key ${timestamp}/
    echo "aws s3 cp * s3://lottery-ticket-pruner/" >> example.sh
    ./example.sh || sudo shutdown -h now
Delete TODO.txt
Bump version to legit first version
Change API token to one scoped to only lottery_ticket_pruner repo
Implement same-sign pruning


DONE
----
Get "pip install" working
    - Basic install
    - Create wheel
    - Add testing support
Enable code coverage report generation
Find better solution than passing around 2 different kinds of prune functions (local and global)
Inspect code coverage misses
Make sure unit tests pass reliably even after multiple invocations
Make sure code can be imported intelligently. "import lottery_ticker_pruner; ltp = LotteryTicketPruner(model)"
Make sure we're doing logging in interesting places
Document the code thoroughly
Figure out why tox isn't showing proper code coverage numbers but `pytest --cov=lottery_ticket_pruner --cov-branch --cov-append --cov-report=term --pyargs lottery_ticket_pruner tests` is
Add copyrights to code
Create example (using MNIST?)
Removed restore_initial_weights() in favor of caller doing model.get_weights(), model.set_weights()
Include original weights in results returned by iterate_prunables()
Consider changing prune_weights() to calc_prune_mask() and apply_pruning() to apply_pruning(model)
Implement 'large_final' pruning strategy
Implement Dynamic Weight Rescaling
Document the example better
Review what all I should leave copyright in and what I shouldn't.
Also check to see if including MIT license in the files is a good idea.
    ANSWER: A single LICENSE file at root is sufficient.
Train using patience setting for paper? Or arbitrary # of epochs?
    ANSWER: Added support for patience callback. Using patience=3 in examples.
See if pruning from transfer learning starting points instead of random weights has benefits
    ANSWER: Yes, it definitely applies to transfer learning too.
Set flake8 max line length to 120 and have it enforce. Reformat code accordingly as needed.
Update example.py from transfer_learning_example.py
Address all outstanding TODOs
Implement inside my training pipeline to see if pruning results for DNNs show promise
Use github to build upon pushes?
Post package to pypi
    Keyword: Keras, pruned, CNN pruning, DNN pruning, lottery ticket pruning
    https://packaging.python.org/guides/using-testpypi/
Make sure package works with python 3.6 thru 3.8
Make sure things work with TF2
Hook up posting to pypi.org
Create shell scripts to run complete matrix of example.py
